
import numpy as np
import torch
from scenario import *
import tqdm

from scenario import signature
from torch.utils.data import Dataset
from tokenizer import tok_encode, token2id


def full_path_examples(number_of_examples: int, max_step: int, max_height: int) -> set[Term]:
    '''
    Generate the rewriting paths and extract the examples of every step. 
    All examples are unique in syntax. 

    Constraints:
        - LHS and RHS are different
    '''
    # Generate the examples
    examples_set : set[Term] = set()

    progress_bar = tqdm.tqdm(range(number_of_examples), desc="Generating examples")

    while True:
        path = gen_example(max_step, max_height).get_inverse(INV_GEN_RULES, INST_VARS)

        for (term, _, _, _, _) in path.path:
            # if LHS = RHS, skip
            if term.args[0] == term.args[1]:
                continue

            if term in examples_set:
                continue

            examples_set.add(term)
            
            progress_bar.update(1)

            if len(examples_set) == number_of_examples:
                return examples_set


class ExampleDataset(Dataset):
    '''
    The supervised learning dataset obtained by reversing the path generated by gen_example.

    Will omit the examples exceeding the max_len.
    '''
    def __init__(self, 
                 number_of_examples: int, 
                 max_step: int,
                 max_height: int,
                 max_len: int = 256):
        
        self.sig = signature
        self.number_of_examples = number_of_examples
        self.max_step = max_step
        self.max_height = max_height

        SOS_ID = token2id['<SOS>']
        EOS_ID = token2id['<EOS>']

        # Generate the examples
        examples_set : set[tuple[tuple[int, ...], tuple[int, ...]]] = set()

        progress_bar = tqdm.tqdm(range(number_of_examples), desc="Generating examples")

        while len(examples_set) < number_of_examples:
            path = gen_example(max_step, max_height).get_inverse(INV_GEN_RULES, INST_VARS)

            for (term, opt, pos, _, given_subst) in path.path:
                # if LHS = RHS, skip
                if term.args[0] == term.args[1]:
                    continue

                # synthesis and encode the prompt
                prompt = term.sig_str(self.sig) + " : " + RULE_NAMES[opt]+ " " + " ".join(str(p) for p in pos) + " " + given_subst.sig_str(self.sig)
                encoded_ids = tuple(tok_encode(prompt))

                # print(prompt)
                
                input = (SOS_ID, ) + encoded_ids
                label = encoded_ids + (EOS_ID, )

                # check the length
                if len(input) > max_len:
                    continue

                example = (input, label)
                if example in examples_set:
                    continue

                examples_set.add(example)
                
                progress_bar.update(1)
        
        self.examples = list(examples_set)

    def __len__(self):
        """
        Returns the size of the dataset.
        """
        return self.number_of_examples

    def __getitem__(self, idx):
        """
        Generates one example.
        """
        return self.examples[idx]
    

def get_collate_fn(device: str = 'cpu'):
    def collate_fn(batch):

        PAD_ID = token2id['<PAD>']
        COLON_ID = token2id[':']

        # Find the longest sequence in the batch
        batch_max_len = max([len(x[0]) for x in batch])

        padded_inputs = []
        padded_labels = []
        masks = []

        for input, label in batch:

            padded_input = list(input) + [PAD_ID] * (batch_max_len - len(input))
            padded_inputs.append(padded_input)

            padded_label = list(label) + [PAD_ID] * (batch_max_len - len(input))
            padded_labels.append(padded_label)

            # only predict the tokens after the colon
            colon_idx = padded_label.index(COLON_ID)
            
            mask = [0] * (colon_idx + 1) + [1] * (len(input) - colon_idx - 1) + [0] * (batch_max_len - len(input))
            masks.append(mask)

        return torch.tensor(padded_inputs, device = device), torch.tensor(padded_labels, device = device), torch.tensor(masks, device = device)
    
    return collate_fn

if __name__ == "__main__":
    dataset = ExampleDataset(2000000, 8, 4)
    print(len(dataset))
    print(dataset[0])
