
import numpy as np
import torch
from scenario import *
import tqdm

from scenario import signature
from torch.utils.data import Dataset
from tokenizer import tok_encode, token2id
import multiprocessing

def full_path_examples(number_of_examples: int, max_step: int, max_height: int) -> set[Term]:
    '''
    Generate the rewriting paths and extract the examples of every step. 
    All examples are unique in syntax. 

    Constraints:
        - LHS and RHS are different
    '''
    # Generate the examples
    examples_set : set[Term] = set()

    progress_bar = tqdm.tqdm(range(number_of_examples), desc="Generating examples")

    while True:
        path = gen_example(max_step, max_height).get_inverse(INV_GEN_RULES, INST_VARS)

        for (term, _, _, _, _) in path.path:
            # if LHS = RHS, skip
            if term.args[0] == term.args[1]:
                continue

            if term in examples_set:
                continue

            examples_set.add(term)
            
            progress_bar.update(1)

            if len(examples_set) == number_of_examples:
                return examples_set



SOS_ID = token2id['<SOS>']
EOS_ID = token2id['<EOS>']

class ExampleDataset(Dataset):
    '''
    The supervised learning dataset obtained by reversing the path generated by gen_example.
    Will omit the examples exceeding the max_len.
    '''
    def __init__(self, 
                 number_of_examples: int, 
                 max_step: int,
                 max_height: int,
                 max_len: int = 256,
                 num_workers: int = 16):
        
        self.sig = signature
        self.number_of_examples = number_of_examples
        self.max_step = max_step
        self.max_height = max_height
        self.max_len = max_len
        self.num_workers = num_workers

        # Generate examples in parallel
        self.examples = self._generate_examples_in_parallel()


    def _generate_example_subset(self, num_examples) -> list[tuple[torch.Tensor, torch.Tensor]]:
        """
        Generates a subset of examples, to be run in a separate process.
        """
        examples_set = set()

        progress_bar = tqdm.tqdm(range(num_examples), desc="Generating Process")

        while len(examples_set) < num_examples:
            path = gen_example(self.max_step, self.max_height).get_inverse(INV_GEN_RULES, INST_VARS)

            for (term, opt, pos, _, given_subst) in path.path:
                if term.args[0] == term.args[1]:  # Skip if LHS = RHS
                    continue

                # Synthesize and encode the prompt
                prompt = term.sig_str(self.sig) + " : " + RULE_NAMES[opt]+ " " + " ".join(str(p) for p in pos) + " " + given_subst.sig_str(self.sig)
                encoded_ids = tuple(tok_encode(prompt))

                input = (SOS_ID, ) + encoded_ids
                label = encoded_ids + (EOS_ID, )

                # Check the length
                if len(input) > self.max_len:
                    continue

                example = (input, label)
                examples_set.add(example)

                progress_bar.update(1)
        
        return list(examples_set)

    def _generate_examples_in_parallel(self) -> list[tuple[torch.Tensor, torch.Tensor]]:
        """
        Generates examples in parallel using multiprocessing.
        """
        num_examples_per_worker = self.number_of_examples // self.num_workers
        extra_examples = self.number_of_examples % self.num_workers

        with multiprocessing.Pool(processes=self.num_workers) as pool:
            # Create a progress bar for the total number of examples
            with tqdm.tqdm(total=self.num_workers, desc="Process Overview") as progress_bar:
                # Run the example generation across multiple processes
                results = []
                for i in range(self.num_workers):
                    # Give extra examples to some workers to meet exact count
                    worker_examples = num_examples_per_worker + (1 if i < extra_examples else 0)
                    result = pool.apply_async(self._generate_example_subset, args=(worker_examples,))
                    results.append(result)

                # Collect the results and update the progress bar
                examples_list = []
                for result in results:
                    examples_list.extend(result.get())
                    progress_bar.update(1)

        return examples_list


    def __len__(self):
        """
        Returns the size of the dataset.
        """
        return self.number_of_examples

    def __getitem__(self, idx):
        """
        Generates one example.
        """
        return self.examples[idx]
    

def get_collate_fn(device: str = 'cpu'):
    def collate_fn(batch):

        PAD_ID = token2id['<PAD>']

        # Find the longest sequence in the batch
        batch_max_len = max([len(x[0]) for x in batch])

        padded_inputs = []
        padded_labels = []
        loss_masks = []

        for input, label in batch:

            padded_input = list(input) + [PAD_ID] * (batch_max_len - len(input))
            padded_inputs.append(padded_input)

            padded_label = list(label) + [PAD_ID] * (batch_max_len - len(input))
            padded_labels.append(padded_label)
            
            loss_mask = [1] * len(input) + [0] * (batch_max_len - len(input))
            loss_masks.append(loss_mask)

        return torch.tensor(padded_inputs, device = device), torch.tensor(padded_labels, device = device), torch.tensor(loss_masks, device = device)
    
    return collate_fn

if __name__ == "__main__":
    dataset = ExampleDataset(2000000, 8, 4)
    print(len(dataset))
    print(dataset[0])
